<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Training with CloudML • cloudml</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">CloudML for R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/training.html">Training with CloudML</a>
    </li>
    <li>
      <a href="../articles/training.html">Deploying Models</a>
    </li>
    <li>
      <a href="../articles/tuning.html">Hyperparameter Tuning</a>
    </li>
    <li>
      <a href="../articles/storage.html">Google Cloud Storage</a>
    </li>
    <li>
      <a href="../articles/configuration.html">CloudML Configuration</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/cloudml">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Training with CloudML</h1>
            
          </div>

    
    
<div class="contents">
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p>Training models with CloudML uses the following workflow:</p>
<ul>
<li><p>Develop and test an R training script locally</p></li>
<li><p>Submit a job to CloudML to execute your script in the cloud</p></li>
<li><p>Monitor and collect the results of the job</p></li>
<li><p>Tune your model based on the results and repeat training as necessary</p></li>
</ul>
<p>This article describes this workflow in more detail.</p>
</div>
<div id="local-development" class="section level2">
<h2 class="hasAnchor">
<a href="#local-development" class="anchor"></a>Local Development</h2>
<p>Working on a CloudML project always begins with developing a training script that runs on your local machine. This will typically involve using one of these packages:</p>
<ul>
<li><p><a href="https://keras.rstudio.com/">keras</a> — A high-level interface for neural networks, with a focus on enabling fast experimentation.</p></li>
<li><p><a href="https://tensorflow.rstudio.com/tfestimators">tfestimators</a> — High-level implementations of common model types such as regressors and classifiers.</p></li>
<li><p><a href="https://tensorflow.rstudio.com/">tensorflow</a> — Lower-level interface that provides full access to the TensorFlow computational graph.</p></li>
</ul>
<p>There are no special requirements for your training script, however there are a couple of things to keep in mind:</p>
<ol style="list-style-type: decimal">
<li><p>When you train a model on CloudML all of the files in the current working directory are uploaded. Therefore, your training script should be within the current working directory and references to other scripts, data files, etc. should be relative to the current working directory. As a result, the most straightforward way to organize your work on a CloudML application is to use an <a href="https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects">RStudio Project</a>.</p></li>
<li><p>Your training data may be contained within the working directory, or it may be located within Google Cloud Storage. If your training data is large and/or located in cloud storage, the most straightforward workflow for development is to use a local subsample of your data. See the article on <a href="storage.html">Google Cloud Storage</a> for a detailed example of using distinct data for local and CloudML execution contexts, as well as reading data from Google Cloud Storage buckets.</p></li>
</ol>
<p>Once your script is working the way you expect you are ready to submit it as a job to CloudML.</p>
</div>
<div id="submitting-jobs" class="section level2">
<h2 class="hasAnchor">
<a href="#submitting-jobs" class="anchor"></a>Submitting Jobs</h2>
<p>The core unit of work in CloudML is a job. A job consists of a training script and related files (e.g. other scripts, data files, ec.). To submit a job to CloudML you use the <code><a href="../reference/cloudml_train.html">cloudml_train()</a></code> function, passing it the name of the training script to run. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cloudml)
job &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cloudml_train.html">cloudml_train</a></span>(<span class="st">"minst_mlp.R"</span>)</code></pre></div>
<div class="bs-callout bs-callout-warning">
<p>Note that the CloudML training and job management functions described here all work relative to the current working directory of your R session, so you should always switch to the working directory containing your scripts before executing these functions.</p>
</div>
<p>The <code><a href="../reference/cloudml_train.html">cloudml_train()</a></code> function returns a <code>job</code> object. This is a reference to the training job which you can use later to check it’s status, collect it’s output, etc. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_status.html">job_status</a></span>(job)</code></pre></div>
<pre><code> $ createTime    : chr "2017-12-18T20:35:21Z"
 $ etag          : chr "2KRqIbAhzvM="
 $ jobId         : chr "cloudml_2017_12_18_203510175"
 $ startTime     : chr "2017-12-18T20:35:52Z"
 $ state         : chr "RUNNING"
 $ trainingInput :List of 3
  ..$ jobDir        : chr "gs://cedar-card-791/r-cloudml/staging"
  ..$ region        : chr "us-east1"
  ..$ runtimeVersion: chr "1.4"
 $ trainingOutput:List of 1
  ..$ consumedMLUnits: num 0.04

View job in the Cloud Console at:
https://console.cloud.google.com/ml/jobs/cloudml_2017_12_18_203510175?project=cedar-card-791

View logs at:
https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Fcloudml_2017_12_18_203510175&amp;project=cedar-card-791</code></pre>
<p>To interact with jobs you don’t need the <code>job</code> object returned from <code><a href="../reference/cloudml_train.html">cloudml_train()</a></code>. If you call <code><a href="../reference/job_status.html">job_status()</a></code> or with no arguments it will act on the most recently submitted job:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_status.html">job_status</a></span>()   <span class="co"># get status of last job</span></code></pre></div>
</div>
<div id="collecting-job-results" class="section level2">
<h2 class="hasAnchor">
<a href="#collecting-job-results" class="anchor"></a>Collecting Job Results</h2>
<p>You can call <code><a href="../reference/job_collect.html">job_collect()</a></code> at any time to download a job:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_collect.html">job_collect</a></span>()     <span class="co"># collect last job</span>
<span class="kw"><a href="../reference/job_collect.html">job_collect</a></span>(job)  <span class="co"># collect specific job</span></code></pre></div>
<p>Note also that if you are using RStudio v1.1 or higher you’ll be given the to monitor and collect submitted jobs in the background using an RStudio terminal:</p>
<div class="figure">
<img src="images/rstudio-terminal.png" class="screenshot" width="730">
</div>
<p>In this case you don’t need to call <code><a href="../reference/job_collect.html">job_collect()</a></code> explicitly as this will be done from within the background terminal after the job completes.</p>
<p>Once the job is complete it’s results will be downloaded and a report will be automatically displayed:</p>
<div class="figure">
<img src="images/training-run.png" class="screenshot" width="730">
</div>
<div id="training-runs" class="section level3">
<h3 class="hasAnchor">
<a href="#training-runs" class="anchor"></a>Training Runs</h3>
<p>Each training job will produce one or more training runs (it’s typically only a single run, however when doing hyperparmeter turning there will be multiple runs). When you collect a job from CloudML it is automatically downloaded into the <code>runs</code> sub-directory of the current working directory.</p>
<p>You can list all of the runs as a data frame using the <code>ls_runs()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ls_runs</span>()</code></pre></div>
<pre><code># A tibble: 6 x 34
                            run_dir metric_loss metric_acc metric_val_loss metric_val_acc
                              &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;
1 runs/cloudml_2017_12_15_182614794      0.0809     0.9763          0.0889         0.9786
2 runs/cloudml_2017_12_14_183247626      0.0806     0.9770          0.0919         0.9773
3 runs/cloudml_2017_12_14_144048138      0.0786     0.9772          0.0896         0.9777
4 runs/cloudml_2017_12_14_143427111      0.0803     0.9771          0.0940         0.9760
5 runs/cloudml_2017_12_14_124739611      0.0829     0.9766          0.0913         0.9782
6 runs/cloudml_2017_12_14_124625505      0.0805     0.9765          0.0981         0.9766
# ... with 29 more variables: flag_dropout1 &lt;dbl&gt;, flag_dropout2 &lt;dbl&gt;, samples &lt;int&gt;,
#   validation_samples &lt;int&gt;, batch_size &lt;int&gt;, epochs &lt;int&gt;, epochs_completed &lt;int&gt;,
#   metrics &lt;chr&gt;, model &lt;chr&gt;, loss_function &lt;chr&gt;, optimizer &lt;chr&gt;, learning_rate &lt;dbl&gt;,
#   script &lt;chr&gt;, start &lt;dttm&gt;, end &lt;dttm&gt;, completed &lt;lgl&gt;, output &lt;chr&gt;, source_code &lt;chr&gt;,
#   context &lt;chr&gt;, type &lt;chr&gt;, cloudml_console_url &lt;chr&gt;, cloudml_created &lt;dttm&gt;,
#   cloudml_end &lt;dttm&gt;, cloudml_job &lt;chr&gt;, cloudml_log_url &lt;chr&gt;, cloudml_ml_units &lt;dbl&gt;,
#   cloudml_scale_tier &lt;chr&gt;, cloudml_start &lt;dttm&gt;, cloudml_state &lt;chr&gt;</code></pre>
<p>You can view run reports using the <code>view_run()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># view the latest run</span>
<span class="kw">view_run</span>()

<span class="co"># view a specific run</span>
<span class="kw">view_run</span>(<span class="st">"runs/cloudml_2017_12_15_182614794"</span>)</code></pre></div>
<p>There are many tools available to list, filter, and compare training runs. For additional information see the documentation for the <a href="https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html">tfruns package</a>.</p>
</div>
</div>
<div id="managing-jobs" class="section level2">
<h2 class="hasAnchor">
<a href="#managing-jobs" class="anchor"></a>Managing Jobs</h2>
<p>You can enumerate previously submitted jobs using the <code><a href="../reference/job_list.html">job_list()</a></code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_list.html">job_list</a></span>()</code></pre></div>
<pre><code>                        JOB_ID    STATUS             CREATED
1 cloudml_2017_12_18_203510175 SUCCEEDED 2017-12-18 15:35:21
2 cloudml_2017_12_18_202228264    FAILED 2017-12-18 15:22:39
3 cloudml_2017_12_18_201607948 SUCCEEDED 2017-12-18 15:16:18
4 cloudml_2017_12_18_132620918 SUCCEEDED 2017-12-18 08:26:30
5 cloudml_2017_12_15_182614794 SUCCEEDED 2017-12-15 13:26:29
6 cloudml_2017_12_14_183247626 SUCCEEDED 2017-12-14 13:33:04</code></pre>
<p>You can use the <code>JOB_ID</code> field to interact with any of these jobs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_status.html">job_status</a></span>(<span class="st">"cloudml_2017_12_18_203510175"</span>)</code></pre></div>
<p>The <code><a href="../reference/job_stream_logs.html">job_stream_logs()</a></code> function can be used to view the live log of a running job:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_stream_logs.html">job_stream_logs</a></span>(<span class="st">"cloudml_2017_12_18_203510175"</span>)</code></pre></div>
<p>The <code><a href="../reference/job_cancel.html">job_cancel()</a></code> function can be used to cancel a running job:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_cancel.html">job_cancel</a></span>(<span class="st">"cloudml_2017_12_18_203510175"</span>)</code></pre></div>
</div>
<div id="tuning-your-application" class="section level2">
<h2 class="hasAnchor">
<a href="#tuning-your-application" class="anchor"></a>Tuning Your Application</h2>
<p>Tuning your application typically requires choosing and then optimizing a set of hyperparameters that influence your model’s performance. This could include the number and type of layers, units within layers, drop rates, regularization, etc.</p>
<p>You can experiment with hyperparameters on an ad-hoc basis, but in general it’s better to explore them more systematnically. The key to doing this with CloudML is by defining <a href="https://tensorflow.rstudio.com/tools/training_flags.html">training flags</a> within your script and the parameterizing runs using those flags.</p>
<p>For example, you might define the following training flags:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)

FLAGS &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flags</a></span>(
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_integer</a></span>(<span class="st">"dense_units1"</span>, <span class="dv">128</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_numeric</a></span>(<span class="st">"dropout1"</span>, <span class="fl">0.4</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_integer</a></span>(<span class="st">"dense_units2"</span>, <span class="dv">128</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_numeric</a></span>(<span class="st">"dropout2"</span>, <span class="fl">0.3</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_integer</a></span>(<span class="st">"epochs"</span>, <span class="dv">30</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_integer</a></span>(<span class="st">"batch_size"</span>, <span class="dv">128</span>),
  <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flag_numeric</a></span>(<span class="st">"learning_rate"</span>, <span class="fl">0.001</span>)
)</code></pre></div>
<p>Then use the flags in a script as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">input &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_input">layer_input</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">784</span>))
predictions &lt;-<span class="st"> </span>input <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> FLAGS<span class="op">$</span>dense_units1, <span class="dt">activation =</span> <span class="st">'relu'</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dropout">layer_dropout</a></span>(<span class="dt">rate =</span> FLAGS<span class="op">$</span>dropout1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> FLAGS<span class="op">$</span>dense_units2, <span class="dt">activation =</span> <span class="st">'relu'</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dropout">layer_dropout</a></span>(<span class="dt">rate =</span> FLAGS<span class="op">$</span>dropout2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>)

model &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/keras_model">keras_model</a></span>(input, predictions) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/compile">compile</a></span>(
  <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
  <span class="dt">optimizer =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/optimizer_rmsprop">optimizer_rmsprop</a></span>(<span class="dt">lr =</span> FLAGS<span class="op">$</span>learning_rate),
  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
)

history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/fit">fit</a></span>(
  x_train, y_train,
  <span class="dt">batch_size =</span> FLAGS<span class="op">$</span>batch_size,
  <span class="dt">epochs =</span> FLAGS<span class="op">$</span>epochs,
  <span class="dt">verbose =</span> <span class="dv">1</span>,
  <span class="dt">validation_split =</span> <span class="fl">0.2</span>
)</code></pre></div>
<p>Note that instead of literal values for the various hyperparameters we want to vary we now reference members of the FLAGS list returned from the <code><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">flags()</a></code> function.</p>
<p>You can try out different flags by passing a named list of <code>flags</code> to the <code><a href="../reference/cloudml_train.html">cloudml_train()</a></code> function. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/cloudml_train.html">cloudml_train</a></span>(<span class="st">"minst_mlp.R"</span>, <span class="dt">flags =</span> <span class="kw">list</span>(<span class="dt">dropout1 =</span> <span class="fl">0.3</span>, <span class="dt">dropout2 =</span> <span class="fl">0.2</span>))</code></pre></div>
<p>These flags are recorded and become part of the the results recorded:</p>
</div>
<div id="learning-more" class="section level2">
<h2 class="hasAnchor">
<a href="#learning-more" class="anchor"></a>Learning More</h2>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#overview">Overview</a></li>
      <li><a href="#local-development">Local Development</a></li>
      <li><a href="#submitting-jobs">Submitting Jobs</a></li>
      <li><a href="#collecting-job-results">Collecting Job Results</a></li>
      <li><a href="#managing-jobs">Managing Jobs</a></li>
      <li><a href="#tuning-your-application">Tuning Your Application</a></li>
      <li><a href="#learning-more">Learning More</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, JJ Allaire, Kevin Ushey.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
