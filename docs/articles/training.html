<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Training with CloudML • cloudml</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">CloudML for R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/training.html">Training</a>
    </li>
    <li>
      <a href="../articles/tuning.html">Tuning</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/cloudml">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Training with CloudML</h1>
            
          </div>

    
    
<div class="contents">
<p>Cloud Machine Learning Engine enables you to easily run your TensorFlow training applications in the cloud. This page describes that capability and some of the key concepts you’ll need to understand to make the most of your model training.</p>
<div id="how-training-works" class="section level2">
<h2 class="hasAnchor">
<a href="#how-training-works" class="anchor"></a>How training works</h2>
<p>Your training application, implemented in R and TensorFlow, is the core of the training process. Cloud ML Engine runs your trainer on computing resources in the cloud. Here’s an overview of the process:</p>
<ol style="list-style-type: decimal">
<li>You create a TensorFlow application that defines your computation graph and trains your model. Cloud ML Engine has almost no specific requirements of your application during the training process, so you build it as you would to run locally in your development environment.</li>
<li>You get your training and verification data into a source that Cloud ML Engine can access. This usually means putting it in Google Cloud Storage, Cloud Bigtable, or another Google Cloud Platform storage service associated with the same Google Cloud Platform project that you’re using for Cloud ML Engine.</li>
<li>When your application is ready to run, it must be packaged and transferred to a Google Cloud Storage bucket that your project can access. This is automated with <code><a href="../reference/cloudml_train.html">cloudml_train()</a></code>.</li>
<li>The Cloud ML Engine training service sets up resources for your job. It allocates one or more virtual machines (sometimes called training instances) based on your job configuration. Each training instance is set up by:
<ul>
<li>Applying the standard machine image for the version of Cloud ML Engine your job uses.</li>
<li>Loading your trainer package and installing.</li>
<li>Installing any additional packages that <code>packrat</code> detects.</li>
</ul>
</li>
<li>The training service runs your trainer.</li>
<li>You can get information about your running job in three ways:
<ul>
<li>On Stackdriver Logging.</li>
<li>By requesting job details or running log streaming with the gcloud command-line tool.</li>
<li>By programmatically making status requests to the training service using <code><a href="../reference/job_status.html">job_status()</a></code>.</li>
</ul>
</li>
<li>When your trainer succeeds or encounters an unrecoverable error, Cloud ML Engine halts all job processes and cleans up the resources.</li>
</ol>
<p>If you run a distributed TensorFlow job with Cloud ML Engine, you’ll specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify and performs step 4 above on each. Your running trainer on a given node is called a replica. In accordance with the distributed TensorFlow model, each replica in the training cluster is given a single role or task in distributed training:</p>
<ul>
<li>Exactly one replica is designated the master. This task manages the others and reports status for the job as a whole. It’s asserted in the previous list that the training service runs until “your trainer” succeeds or encounters an unrecoverable error. In the distributed case, it is the status of the master replica that signals the overall job status. If you are running a single-process job, the sole replica is the master for the job.</li>
<li>One or more replicas may be designated as workers. These replicas do their portion of the work as you designate in your trainer.</li>
<li>One or more replicas may be designated as parameter servers. These replicas coordinate shared model state between the workers.</li>
</ul>
</div>
<div id="the-typical-case" class="section level2">
<h2 class="hasAnchor">
<a href="#the-typical-case" class="anchor"></a>The typical case</h2>
<p>There are steps in the description above where you might assume that a machine learning service would intervene or control processing but where Cloud ML Engine doesn’t. The training service is designed to have as little an impact on your trainer as possible. This means you can focus on the TensorFlow code that makes the model you want instead of being confined by a rigid structure. Essentially this means that Cloud ML Engine doesn’t know or take interest in your application’s implementation.</p>
<p>While it’s true that the training service imposes almost no restriction on your trainer’s architecture, that doesn’t mean that there isn’t any guidance to follow. Most machine learning trainers:</p>
<ul>
<li>Provide a way to get training data and evaluation data.</li>
<li>Process data instances in batches.</li>
<li>Use evaluation data to test the accuracy of the model (how often it predicts the right value).</li>
<li>Provide a way to output checkpoints at intervals in the process to get a snapshot of the model’s progress.</li>
<li>Provide a way to export the trained model when the trainer finishes.</li>
</ul>
</div>
<div id="configuring-your-trainer" class="section level2">
<h2 class="hasAnchor">
<a href="#configuring-your-trainer" class="anchor"></a>Configuring your trainer</h2>
<p>If you’ve never made a Python package before, this process can feel daunting. The good news is that you can rely on the gcloud command-line tool to do the heavy lifting for you. This section covers some of the specifics in more detail.</p>
<div id="configuration" class="section level3">
<h3 class="hasAnchor">
<a href="#configuration" class="anchor"></a>Configuration</h3>
<p>A <code>cloudml.yml</code> file must be specified in the current directory to configure: project, account, region, runtime and storage parameters used to configure Cloud ML while traininig.</p>
<p>A simple configuration file looks as follows:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">gcloud:</span>
  <span class="fu">project         :</span><span class="at"> </span><span class="st">"project-name"</span>
  <span class="fu">account         :</span><span class="at"> </span><span class="st">"account@domain.com"</span>
  <span class="fu">region          :</span><span class="at"> </span><span class="st">"us-central1"</span>
  <span class="fu">runtime-version :</span><span class="at"> </span><span class="st">"1.4"</span>

<span class="fu">cloudml:</span>
  <span class="fu">storage         :</span><span class="at"> </span><span class="st">"gs://project-name/mnist"</span></code></pre></div>
</div>
<div id="scale-tier" class="section level3">
<h3 class="hasAnchor">
<a href="#scale-tier" class="anchor"></a>Scale tier</h3>
<p>You must tell Cloud ML Engine the number and type of machines to run your training job on. To make the process easier, you can pick from a set of predefined cluster specifications called scale tiers.</p>
<p>You can read more about the availaible tiers in <a href="https://cloud.google.com/ml-engine/docs/training-overview#scale_tier">Trainning Overview - Scale Tiers</a>.</p>
</div>
<div id="region" class="section level3">
<h3 class="hasAnchor">
<a href="#region" class="anchor"></a>Region</h3>
<p>Google Cloud Platform uses <a href="https://cloud.google.com/compute/docs/regions-zones/regions-zones">zones and regions</a> to define the geographic locations of physical computing resources. Cloud ML Engine uses regions to designate its processing. When you run a training job, you specify the region that you want it to run in.</p>
<p>If you store your training dataset on Google Cloud Storage, you should run your training job in the same region as the bucket you’re using. If you must run your job in a different region from your data bucket, your job may take longer.</p>
<p>Additional configuration options can be found udner <a href="https://cloud.google.com/ml-engine/docs/training-overview">Trainning Overview</a>.</p>
</div>
<div id="dependencies" class="section level3">
<h3 class="hasAnchor">
<a href="#dependencies" class="anchor"></a>Dependencies</h3>
<p>All package dependencies are automatically detected with a <a href="http://rstudio.github.io/packrat/">packrat</a> snapshot; this snapshot contains a list of packages and versions that, once training starts, is used to restore the state of the packages in each Cloud ML instance.</p>
<div id="caching" class="section level4">
<h4 class="hasAnchor">
<a href="#caching" class="anchor"></a>Caching</h4>
<p>Caching R package dependencies is enabled by default, this allows subsequent training runs to save several minutes of initialization time. The package cache is stored under <code>storage</code>; it can be disabled by setting this property to <code>false</code> in <code>cloudml.yml</code>:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">cloudml:</span>
  <span class="fu">storage         :</span><span class="at"> </span><span class="st">"gs://project-name/mnist"</span>
  <span class="fu">cache           :</span><span class="at"> false</span></code></pre></div>
<p>The cache can also be shared among many projects by setting the <code>cache</code> path as follows:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">cloudml:</span>
  <span class="fu">storage         :</span><span class="at"> </span><span class="st">"gs://project-name/mnist"</span>
  <span class="fu">cache           :</span><span class="at"> </span><span class="st">"gs://project-name/cache"</span></code></pre></div>
</div>
</div>
</div>
<div id="deploying-your-trainer" class="section level2">
<h2 class="hasAnchor">
<a href="#deploying-your-trainer" class="anchor"></a>Deploying your trainer</h2>
<p>To package and deploy youy job, simply run:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cloudml)
job &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cloudml_train.html">cloudml_train</a></span>()</code></pre></div>
<p>this will start the training process and return a job object you can use to collect the training resuts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/job_collect.html">job_collect</a></span>(job)</code></pre></div>
<p>To view the run results, you can use <code>tfruns</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tfruns)
<span class="kw"><a href="http://www.rdocumentation.org/packages/tfruns/topics/view_run">view_run</a></span>()</code></pre></div>
<p>To understand additional job related operations, please consult the <a href="guides-jobs.html">Jobs Guide</a>.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#how-training-works">How training works</a></li>
      <li><a href="#the-typical-case">The typical case</a></li>
      <li><a href="#configuring-your-trainer">Configuring your trainer</a></li>
      <li><a href="#deploying-your-trainer">Deploying your trainer</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, JJ Allaire, Kevin Ushey.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
