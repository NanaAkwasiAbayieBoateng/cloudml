---
title: "Deploying Models"
output: 
  rmarkdown::html_vignette: default
vignette: >
  %\VignetteIndexEntry{Deploying Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
type: docs
repo: https://github.com/rstudio/cloudml
menu:
  main:
    name: "Deploying Models"
    identifier: "tools-cloudml-deployment"
    parent: "cloudml-top"
    weight: 50
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

You can host your trained machine learning models in the cloud and use the Cloud ML prediction service to infer target values for new data. This page discusses model hosting and prediction and introduces considerations you should keep in mind for your projects.

## Model Deployment

Cloud ML Engine can host your models so that you can get predictions from them in the cloud. The process of hosting a saved model is called deployment. The prediction service manages the infrastructure needed to run your model at scale, and makes it available for online and batch prediction requests. This section describes model deployment.

### Exporting a SavedModel

The Cloud ML prediction service makes use of models exported through the
`export_savedmodel()` function which is available for models created using the [tensorflow](https://tensorflow.rstudio.com/tensorflow/), [keras](https://tensorflow.rstudio.com/keras/) and
[tfestimators](https://tensorflow.rstudio.com/tfestimators/) packages or any other tool that support the [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver) interface.

For instance, we can define and export a simple model by running:

```{r}
library(tensorflow)
library(cloudml)

# define output folder
model_dir <- tempfile()

# define simple string-based tensor operations
sess <- tf$Session()
name <- tf$placeholder(tf$string)
hello <- tf$string_join(inputs = c("Hello, ", name, "!"))

# export as a savedmodel
export_savedmodel(
  sess,
  model_dir,
  inputs = list(name = name),
  outputs = list(hello = hello)
)
```


### Deploying the Model

Deployment is performed through `cloudml_deploy()` which uses the same `gcloud`
and `cloudml` configuration concepts used while training. We can
train any exported model by running:

```{r eval=F}
cloudml_deploy(model_dir, name = "hello_world")
```
```
Model created and available in https://console.cloud.google.com/mlengine/models/hello_world
```

Notice that models make use of unique names and versions which can be specified
using the `name` and `version` parameters in `cloudml_deploy()`.

## Prediction

Once a model is deployed, predictions can be performed by providing a list of inputs into
`cloudml_predict()`:

```{r eval=F}
cloudml_predict(
  name = "hello_world",
  list(
    list(name = "cloudml"),
    list(name = "tensorflow")
  )
)
```

```
$predictions
               hello
1    Hello, cloudml!
2 Hello, tensorflow!
```

For additional information visit [Google Cloud Platform - Prediction Basics](https://cloud.google.com/ml-engine/docs/prediction-overview)
