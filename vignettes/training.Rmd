---
title: "Training with CloudML"
output: 
  rmarkdown::html_vignette: default
vignette: >
  %\VignetteIndexEntry{Training with CloudML}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Overview

Training models with CloudML uses the following workflow:

- Develop and test an R training script locally

- Submit a job to CloudML to execute your script in the cloud

- Monitor and collect the results of the job

- Tune your model based on the results and repeat training as necessary

This article describes this workflow in more detail.

## Local Development

Working on a CloudML project always begins with developing a training script that runs on your local machine. This will typically involve using some combination of these packages:

- [keras](https://keras.rstudio.com/)

- [tfestimators](https://tensorflow.rstudio.com/tfestimators)

- [tensorflow](https://tensorflow.rstudio.com/) 

### Project / Working Directory

CloudML training applications consist of a single directory that in turn includes a training script. You can include additional script, configuration, data, or other files within your application simply by having them within the working directory where you execute `cloudml_train()` from. The most straightforward way to organize your work on a CloudML application is to use an [RStudio Project](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).

### Local vs. CloudML Data

One thing you will almost certainly want to do during local development is work against a smaller subsample of your dataset. You can write a script that uses one set of data locally and another set of data on CloudML by using [training flags](https://tensorflow.rstudio.com/tools/training_flags.html) along with the [Google Cloud Storage](storage.html) functions provided by the **cloudml** package.

See the article on [Google Cloud Storage](storage.html) for a detailed example of using distinct data for local and CloudML execution contexts, as well as reading data from Google Cloud Storage buckets.

Once your script is working the way you expect you are ready to submit it as a job to CloudML.

## Submitting Jobs

The core unit of work in CloudML is a job. A job consists of a training script and related files (e.g. other scripts, data files, ec.). To submit a job to CloudML you use the `cloudml_train()` function, passing it the name of the training script to run. For example:

```{r}
job <- cloudml_train("minst_mlp.R")
```

<div class="bs-callout bs-callout-warning">
Note that the CloudML training and job management functions described here all work relative to the current working directory of your R session, so you should always switch to the working directory containing your scripts before executing these functions.
</div>

The `cloudml_train()` function returns a `job` object. This is a reference to the training job which you can use later to check it's status, collect it's output, etc. For example:

```{r}
job_status(job)
```
```
 $ createTime    : chr "2017-12-18T20:35:21Z"
 $ etag          : chr "2KRqIbAhzvM="
 $ jobId         : chr "cloudml_2017_12_18_203510175"
 $ startTime     : chr "2017-12-18T20:35:52Z"
 $ state         : chr "RUNNING"
 $ trainingInput :List of 3
  ..$ jobDir        : chr "gs://cedar-card-791/r-cloudml/staging"
  ..$ region        : chr "us-east1"
  ..$ runtimeVersion: chr "1.4"
 $ trainingOutput:List of 1
  ..$ consumedMLUnits: num 0.04

View job in the Cloud Console at:
https://console.cloud.google.com/ml/jobs/cloudml_2017_12_18_203510175?project=cedar-card-791

View logs at:
https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Fcloudml_2017_12_18_203510175&project=cedar-card-791
```

To interact with jobs you don't need the `job` object returned from `cloudml_train()`. If you call `job_status()` or with no arguments it will act on the most recently submitted job:

```{r}
job_status()   # get status of last job
```

## Collecting Job Results 

You can call `job_collect()` at any time to download a job:

```{r}
job_collect(job)
```

Note also that if you are using RStudio v1.1 or higher you'll be given the to monitor and collect submitted jobs in the background using an RStudio terminal:

![](images/rstudio-terminal.png){.screenshot width=730px}

In this case you don't need to call `job_collect()` explicitly as this will be done from within the background terminal after the job completes.

Once the job is complete it's results will be downloaded and a report will be automatically displayed:

![](images/training-run.png){.screenshot width=730px}

### Training Runs

Each training job will produce one or more training runs (it's typically only a single run, however when doing hyperparmeter turning there will be multiple runs). When you collect a job from CloudML it is automatically downloaded into the `runs` sub-directory of the current working directory.

You can list all of the runs as a data frame using the `ls_runs()` function:

```{r}
ls_runs()
```
```
# A tibble: 6 x 34
                            run_dir metric_loss metric_acc metric_val_loss metric_val_acc
                              <chr>       <dbl>      <dbl>           <dbl>          <dbl>
1 runs/cloudml_2017_12_15_182614794      0.0809     0.9763          0.0889         0.9786
2 runs/cloudml_2017_12_14_183247626      0.0806     0.9770          0.0919         0.9773
3 runs/cloudml_2017_12_14_144048138      0.0786     0.9772          0.0896         0.9777
4 runs/cloudml_2017_12_14_143427111      0.0803     0.9771          0.0940         0.9760
5 runs/cloudml_2017_12_14_124739611      0.0829     0.9766          0.0913         0.9782
6 runs/cloudml_2017_12_14_124625505      0.0805     0.9765          0.0981         0.9766
# ... with 29 more variables: flag_dropout1 <dbl>, flag_dropout2 <dbl>, samples <int>,
#   validation_samples <int>, batch_size <int>, epochs <int>, epochs_completed <int>,
#   metrics <chr>, model <chr>, loss_function <chr>, optimizer <chr>, learning_rate <dbl>,
#   script <chr>, start <dttm>, end <dttm>, completed <lgl>, output <chr>, source_code <chr>,
#   context <chr>, type <chr>, cloudml_console_url <chr>, cloudml_created <dttm>,
#   cloudml_end <dttm>, cloudml_job <chr>, cloudml_log_url <chr>, cloudml_ml_units <dbl>,
#   cloudml_scale_tier <chr>, cloudml_start <dttm>, cloudml_state <chr>
```
You can view run reports using the `view_run()` function:

```{r}
# view the latest run
view_run()

# view a specific run
view_run("runs/cloudml_2017_12_15_182614794")
```

There are many tools available to list, filter, and compare training runs. For additional information see the documentation for the [tfruns package](https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html).


## Managing Jobs 

You can enumerate previously submitted jobs using the `job_list()` function:

```{r}
job_list()
```
```
                        JOB_ID    STATUS             CREATED
1 cloudml_2017_12_18_203510175 SUCCEEDED 2017-12-18 15:35:21
2 cloudml_2017_12_18_202228264    FAILED 2017-12-18 15:22:39
3 cloudml_2017_12_18_201607948 SUCCEEDED 2017-12-18 15:16:18
4 cloudml_2017_12_18_132620918 SUCCEEDED 2017-12-18 08:26:30
5 cloudml_2017_12_15_182614794 SUCCEEDED 2017-12-15 13:26:29
6 cloudml_2017_12_14_183247626 SUCCEEDED 2017-12-14 13:33:04
```

You can use the `JOB_ID` field to interact with any of these jobs:

```{r}
job_status("cloudml_2017_12_18_203510175")
```

The `job_stream_logs()` function can be used to view the live log of a running job:

```{r}
job_stream_logs("cloudml_2017_12_18_203510175")
```

The `job_cancel()` function can be used to cancel a running job:

```{r}
job_cancel("cloudml_2017_12_18_203510175")
```

## Tuning Your Application






## Learning More







